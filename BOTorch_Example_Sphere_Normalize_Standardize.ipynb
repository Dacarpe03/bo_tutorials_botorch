{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BOTorch tutorial\n",
        "Adapted of https://www.youtube.com/watch?v=BQ4kVn-Rt84"
      ],
      "metadata": {
        "id": "Ut_7qUec_fPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we install BOTorch"
      ],
      "metadata": {
        "id": "lUHXZaZ-EBaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install botorch"
      ],
      "metadata": {
        "id": "DRylSzQ1EC0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "aLB1XvRk_cp0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_k6vXoy_Y4p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective function: Sphere 1D very steep. Obviously, we could use other methods here as it is convex optimization, but it is just a toy problem.\n",
        "\n",
        "$f(x) = 1000000 x^2$"
      ],
      "metadata": {
        "id": "sO5iEVhA_n8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_function(individuals):\n",
        "  result = []\n",
        "  for x in individuals:\n",
        "    result.append(1e7*x[0]**2)\n",
        "  return torch.tensor(result)"
      ],
      "metadata": {
        "id": "pBQXY7oZ_o3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print objective function that we want to maximize."
      ],
      "metadata": {
        "id": "F6FwE1eYABoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "lower_bound = -0.1\n",
        "upper_bound = 0.1\n",
        "x = np.linspace(lower_bound, upper_bound, 100)\n",
        "x_new = x.reshape((100,-1))\n",
        "z = target_function(x_new)\n",
        "\n",
        "data = go.Scatter(x=x, y=z, line_color=\"#FE73FF\")\n",
        "\n",
        "fig = go.Figure(data=data)\n",
        "fig.update_layout(title=\"Objective function\", xaxis_title=\"input\", yaxis_title=\"output\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ZlqngMKbAEA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate some data. Example of 10 random points from the input space, normalized into the range."
      ],
      "metadata": {
        "id": "H3VlN1QSBIJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = torch.rand(10000, 1) / 5 - upper_bound\n",
        "print(train_x.min())\n",
        "print(train_x.max())"
      ],
      "metadata": {
        "id": "-FzuqL7TBJSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we compute the latent function $f(x)$. The true evaluation would be contaminated. $y = f(x) + \\epsilon \\quad s.t. \\quad \\epsilon \\approx N(0, \\sigma)$"
      ],
      "metadata": {
        "id": "FSg7ptujBZkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exact_obj = target_function(train_x).unsqueeze(-1)\n",
        "exact_obj"
      ],
      "metadata": {
        "id": "cI-7IyqPBOru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see which is the best observed value so far. Assuming minimization."
      ],
      "metadata": {
        "id": "WDSXYHFFBnI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_observed_value = exact_obj.min().item()\n",
        "best_observed_value"
      ],
      "metadata": {
        "id": "LjNExoa9BpmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wrap all of the previous code into a function to be used freely"
      ],
      "metadata": {
        "id": "ibI-21seCe6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_initial_data(n=10):\n",
        "  train_x = torch.rand(n, 1, dtype=torch.double) / 5 - upper_bound\n",
        "  exact_obj = target_function(train_x).unsqueeze(-1)\n",
        "  best_observed_value = exact_obj.min().item()\n",
        "  return train_x, exact_obj, best_observed_value"
      ],
      "metadata": {
        "id": "QX6Y4KY4ChtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_initial_data(20)"
      ],
      "metadata": {
        "id": "joClmvvSCz5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now invoke this function to start the BO iteration and set the bounds of the 1-D $f(x) : x \\in [-0.1,0.1]$"
      ],
      "metadata": {
        "id": "q8merVh3DEZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_x, init_y, best_init_y = generate_initial_data(20)\n",
        "bounds = torch.tensor([[lower_bound], [upper_bound]]) #bounds for 2D: torch.tensor([[0., 1.], [10.,2.]]) "
      ],
      "metadata": {
        "id": "mqXQD_GxDGxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set which model and which likelihood will we use. In our case we will use a classic Gaussian process and compute its hyper-parameters using the exact marginal log likelihood (which can produce overfitting when points are reduced but well...). We will not normalize the inputs here nor standardize the outputs, so the hyper-parameters of the GP kernel set by default are not going to be a good prior for this function, and this will significantly hurt the optimization, as we will see. "
      ],
      "metadata": {
        "id": "Feti3NQ6DlKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.models import SingleTaskGP, ModelListGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "\n",
        "single_model = SingleTaskGP(init_x, init_y)\n",
        "mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)"
      ],
      "metadata": {
        "id": "cNz3ac16Djuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model is declared, we fit the previous points with the Gaussian process setting its hyperparameters via Exact Marginal log likelihood of the points. The output shows the default covariance function used by the GP and its hyper-hyperparameters. It also shows the Gaussian likelihood used and the homoskedastic noise added to the Matern Kernel to capture the noise of the data. "
      ],
      "metadata": {
        "id": "NNgRjAMYEc75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch import fit_gpytorch_model\n",
        "fit_gpytorch_model(mll)"
      ],
      "metadata": {
        "id": "LvruM2c7ElOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we declare the acquisition function that is going to be computed using the predictive distribution of the previous Gaussian process in all the input space. We will use the expected improvement"
      ],
      "metadata": {
        "id": "S8bMLqQyFJF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.acquisition.analytic import ExpectedImprovement #use the noisy version if the problem has noise\n",
        "\n",
        "EI = ExpectedImprovement(model=single_model, best_f=best_init_y)"
      ],
      "metadata": {
        "id": "P7TUF0XhFOkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now optimize the acquisition function, all the hyper parameters here are a good heuristic default to try and find the global optima of the acquisition function\n",
        "\n"
      ],
      "metadata": {
        "id": "CeBGVaMIFnjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "candidates, _ = optimize_acqf(acq_function=EI, bounds=bounds, q=1, num_restarts=200, raw_samples=512, options={\"batch_limit\": 5, \"maxiter\": 200})\n",
        "\n",
        "candidates"
      ],
      "metadata": {
        "id": "9qvn-98cFnwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the code of an iteration so we just put it in a loop. To do so: We just wrap previous code into a function."
      ],
      "metadata": {
        "id": "7yQWkqbwGQoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_points(init_x, init_y, best_init_y, bounds, n_points=1):\n",
        "  single_model = SingleTaskGP(init_x, init_y)\n",
        "  mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
        "  fit_gpytorch_model(mll)\n",
        "\n",
        "  EI = ExpectedImprovement(model=single_model, best_f=best_init_y, maximize=False)\n",
        "  candidates, _ = optimize_acqf(acq_function=EI, bounds=bounds, q=n_points, num_restarts=200, raw_samples=512, options={\"batch_limit\": 5, \"maxiter\": 200})\n",
        "\n",
        "  return candidates\n"
      ],
      "metadata": {
        "id": "SZXLpbLSGQta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We test the function"
      ],
      "metadata": {
        "id": "BG-WloyRHRox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_next_points(init_x, init_y, best_init_y, bounds, n_points=1)"
      ],
      "metadata": {
        "id": "rLK5VJQ8HREn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we embed the previous code into the Bayesian optimization loop"
      ],
      "metadata": {
        "id": "5lPBXD0ZHex0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations=2\n",
        "\n",
        "init_x, init_y, best_init_y = generate_initial_data(20)\n",
        "bounds = torch.tensor([[lower_bound], [upper_bound]])\n",
        "\n",
        "for i in range(n_iterations):\n",
        "  print(f\"Number of iterations done: {i}\")\n",
        "  new_candidates = get_next_points(init_x, init_y, best_init_y, bounds, 1)\n",
        "  new_results = target_function(new_candidates).unsqueeze(-1)\n",
        "\n",
        "  print(f\"New candidates are: {new_candidates}\")\n",
        "  init_x = torch.cat([init_x, new_candidates])\n",
        "  init_y = torch.cat([init_y, new_results])\n",
        "\n",
        "  best_init_y = init_y.max().item()\n",
        "  print(f\"Best point performs this way: {best_init_y}\")\n"
      ],
      "metadata": {
        "id": "fQ1EGN1WHe-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the best observed result of the optimization. We can see in the previous figure how the result is exactly the maximum. The optimization has been successful."
      ],
      "metadata": {
        "id": "k0VhjwcLJTIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best observed result: {best_init_y}\")\n",
        "best_candidate = init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0]\n",
        "print(f\"Best location of observed result: {best_candidate}\")"
      ],
      "metadata": {
        "id": "fubEczeuJVb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_objective_function(best_candidate, iteration, l_bound=0, h_bound=1):\n",
        "  x = np.linspace(l_bound, h_bound, 100)\n",
        "  x_new = x.reshape((100,-1))\n",
        "  z = target_function(x_new)\n",
        "\n",
        "  data = go.Scatter(x=x, y=z, line_color=\"#FE73FF\")\n",
        "\n",
        "  fig = go.Figure(data=data)\n",
        "  fig.update_layout(title=\"Objective function. Iteration \" + str(iteration), xaxis_title=\"input\", yaxis_title=\"output\")\n",
        "  fig.add_vline(x=best_candidate, line_width=3, line_color=\"red\")\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "print_objective_function(best_candidate, 1, l_bound=lower_bound, h_bound=upper_bound)"
      ],
      "metadata": {
        "id": "XS4zdkHyjGWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.linspace(lower_bound, upper_bound, steps=100)\n",
        "x_test = torch.tensor([x[0]]).unsqueeze(-1)\n",
        "EI = qExpectedImprovement(model=single_model, best_f=best_init_y)\n",
        "EI(x_test)"
      ],
      "metadata": {
        "id": "E2xb2ZF7uAFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot the acquisition function, with its maximum, which is the point suggested to be evaluated in the next iteration"
      ],
      "metadata": {
        "id": "bINYQFghHpLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_acquisition_function(single_model, best_init_y, l_bound=-2., h_bound=10., resolution=1000):\n",
        "  linspace = torch.linspace(l_bound, h_bound, steps=resolution)\n",
        "  x_test = torch.tensor([linspace[0]]).unsqueeze(-1)\n",
        "  EI = ExpectedImprovement(model=single_model, best_f=best_init_y, maximize=False)\n",
        "  result = []\n",
        "  for x in linspace:\n",
        "    x_test = torch.tensor([x]).unsqueeze(-1)\n",
        "    result.append(EI(x_test))\n",
        "  return torch.tensor(result)"
      ],
      "metadata": {
        "id": "VKLPtRcDwl0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_acquisition_function(acq_fun, iteration, l_bound=-2., h_bound=10., resolution=1000, suggested=None):\n",
        "  x = torch.linspace(l_bound, h_bound, steps=resolution).detach().numpy()\n",
        "  x_new = x.reshape((resolution,-1))\n",
        "  z = acq_fun\n",
        "  max_acq_fun = x[((acq_fun == acq_fun.max().item()).nonzero(as_tuple=True)[0])]\n",
        "  data = go.Scatter(x=x, y=z, line_color=\"yellow\")\n",
        "\n",
        "  fig = go.Figure(data=data)\n",
        "  fig.update_layout(title=\"Expected Improvement acquisition function. Iteration \" + str(iteration), xaxis_title=\"input\", yaxis_title=\"output\")\n",
        "  if(suggested==None):\n",
        "    fig.add_vline(x=max_acq_fun, line_width=3, line_color=\"red\")\n",
        "  else:\n",
        "    fig.add_vline(x=float(suggested[0][0]), line_width=3, line_color=\"red\")\n",
        "  fig.show()\n",
        "  "
      ],
      "metadata": {
        "id": "714LAje5xPXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acq_fun = compute_acquisition_function(single_model, best_init_y, l_bound=lower_bound, h_bound=upper_bound)\n",
        "print_acquisition_function(acq_fun, 1)"
      ],
      "metadata": {
        "id": "U453GBQ3uALD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can as well plot the GP predictive mean and standard deviation, its predictive distribution, for all the input space."
      ],
      "metadata": {
        "id": "L_MimbgPHyXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_predictive_distribution(single_model, best_init_y, l_bound=-2., h_bound=10., resolution=1000):\n",
        "  linspace = torch.linspace(l_bound, h_bound, steps=resolution)\n",
        "  x_test = torch.tensor([linspace[0]]).unsqueeze(-1)\n",
        "  result = []\n",
        "  variances = []\n",
        "  for x in linspace:\n",
        "    x_test = torch.tensor([x]).unsqueeze(-1)\n",
        "    result.append(single_model.posterior(x_test).mean)\n",
        "    variances.append(single_model.posterior(x_test).variance)\n",
        "  return torch.tensor(result), torch.tensor(variances)"
      ],
      "metadata": {
        "id": "OPoMt0UPILAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_predictive_mean(predictive_mean, predictive_variance, iteration, l_bound=-2., h_bound=10., resolution=1000, suggested=None, old_obs=[], old_values=[]):\n",
        "  x = torch.linspace(l_bound, h_bound, steps=resolution).detach().numpy()\n",
        "  x_new = x.reshape((resolution,-1))\n",
        "  z = predictive_mean\n",
        "  max_predictive_mean = x[((predictive_mean == predictive_mean.max().item()).nonzero(as_tuple=True)[0])]\n",
        "\n",
        "  fig = go.Figure()\n",
        "\n",
        "  fig.add_trace(go.Scatter(x=x, y= predictive_mean + np.sqrt(predictive_variance),\n",
        "                                     mode='lines',\n",
        "                                     line=dict(color=\"#19D3F3\",width =0.1),\n",
        "                                     name='upper bound'))\n",
        "  fig.add_trace(go.Scatter(x=x, y= predictive_mean,\n",
        "                         mode='lines',\n",
        "                         line=dict(color=\"blue\"),\n",
        "                         fill='tonexty',\n",
        "                         name='predictive mean'))\n",
        "  fig.add_trace(go.Scatter(x=x, y= predictive_mean - np.sqrt(predictive_variance),\n",
        "                         mode='lines',\n",
        "                         line=dict(color=\"blue\", width =0.1),\n",
        "                         fill='tonexty',\n",
        "                         name='lower bound'))\n",
        "  \n",
        "  \n",
        "  \n",
        "  fig.update_layout(title=\"GP Predictive distribution. Iteration \" + str(iteration), xaxis_title=\"input\", yaxis_title=\"output\", showlegend=False)\n",
        "\n",
        "  if(suggested==None):\n",
        "    fig.add_vline(x=max_predictive_mean, line_width=3, line_color=\"red\")\n",
        "  else:\n",
        "    fig.add_vline(x=float(suggested[0][0]), line_width=3, line_color=\"red\")  \n",
        "\n",
        "  if(len(old_obs)>0):\n",
        "    fig.add_trace(go.Scatter(x=old_obs, y=old_values, mode = 'markers', marker_color=\"black\", marker_size=10))\n",
        "\n",
        "  fig.show()"
      ],
      "metadata": {
        "id": "uwnp9gtaIQ-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictive_mean, predictive_variance = compute_predictive_distribution(single_model, best_init_y, l_bound=lower_bound, h_bound=upper_bound)\n",
        "print_predictive_mean(predictive_mean, predictive_variance, 1, l_bound=lower_bound, h_bound=upper_bound)"
      ],
      "metadata": {
        "id": "0SejE404H6em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can embed all this logic into the BO loop to have visualizations of the objective function, GP predictive distribution and acquisition function in every iteration."
      ],
      "metadata": {
        "id": "3as3CZOVQUCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_functions(single_model, best_init_y, best_candidate, candidate_acq_fun, iteration, previous_observations, previous_values, bounds):\n",
        "  predictive_mean, predictive_variance = compute_predictive_distribution(single_model, best_init_y, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "  print_predictive_mean(predictive_mean, predictive_variance, iteration, suggested=candidate_acq_fun, old_obs=previous_observations, old_values=previous_values, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "  acq_fun = compute_acquisition_function(single_model, best_init_y, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "  print_acquisition_function(acq_fun, iteration, suggested=candidate_acq_fun, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "  print_objective_function(best_candidate, iteration, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "\n",
        "def get_next_points_and_visualize(init_x, init_y, best_init_y, bounds, iteration, previous_observations, previous_values, n_points=1):\n",
        "  single_model = SingleTaskGP(init_x, init_y)\n",
        "  mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
        "  fit_gpytorch_model(mll)\n",
        "\n",
        "  EI = ExpectedImprovement(model=single_model, best_f=best_init_y, maximize=False)\n",
        "  \n",
        "  candidates, _ = optimize_acqf(acq_function=EI, bounds=bounds, q=n_points, num_restarts=200, raw_samples=512, options={\"batch_limit\": 5, \"maxiter\": 200})\n",
        "  best_candidate = init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0]\n",
        "\n",
        "  visualize_functions(single_model, best_init_y, best_candidate, candidates, iteration, previous_observations, previous_values, bounds)\n",
        "\n",
        "  return candidates"
      ],
      "metadata": {
        "id": "ozDHlSM-SmDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations=10\n",
        "\n",
        "init_x, init_y, best_init_y = generate_initial_data(20)\n",
        "bounds = torch.tensor([[lower_bound], [upper_bound]])\n",
        "\n",
        "candidates=[]\n",
        "results=[]\n",
        "for i in range(n_iterations):\n",
        "  print(f\"Number of iterations done: {i}\")\n",
        "  new_candidates = get_next_points_and_visualize(init_x, init_y, best_init_y, bounds, i, candidates, results, 1)\n",
        "  new_results = target_function(new_candidates).unsqueeze(-1)\n",
        "\n",
        "  print(f\"New candidates are: {new_candidates}\")\n",
        "  init_x = torch.cat([init_x, new_candidates])\n",
        "  init_y = torch.cat([init_y, new_results])\n",
        "\n",
        "  best_init_y = init_y.min().item()\n",
        "  print(f\"Best point performs this way: {best_init_y}\")\n",
        "  candidates.append(float(new_candidates[0][0]))\n",
        "  results.append(float(new_results[0][0]))"
      ],
      "metadata": {
        "id": "xM0l_oxWR6dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to fix the previous behaviour normalizing and standardizing both inputs and outputs into the unit cube to avoid the previously seen weird behaviour of the GP and, consequently, of the acquisition function. We import both utilities from botorch."
      ],
      "metadata": {
        "id": "fc_Ch3Ka55z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.utils.transforms import standardize, normalize, unnormalize"
      ],
      "metadata": {
        "id": "qgig5i076FB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now test the normalize function, that basically does\n",
        "\n",
        "$\\mathbb{R}^1 \\in [-0.1 , 0.1] \\to \\mathbb{R}^1 \\in [0 , 1]$ \n",
        "\n",
        "Easy stuff. Bounds are set before, but would need to be set for another problem according to its particular details."
      ],
      "metadata": {
        "id": "iwW0BCrQ9EaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = generate_initial_data(20)[0]\n",
        "print(\"Inputs without normalization: \" + str(inputs))\n",
        "print(\"Normalized inputs: \" + str(normalize(inputs, bounds=bounds)))"
      ],
      "metadata": {
        "id": "QFzPsR9u9UFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us standardize (mean=0, std=1) the outputs. Watch out! Standardizing is not the same as normalizing. BOTorch assumes that observations are standardized and that inputs are normalized. Let us make BOTorch happy. Here goes the code for standarization, how well does it do it!!!"
      ],
      "metadata": {
        "id": "yZzY17Mk9wPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = generate_initial_data(20)[1]\n",
        "print(\"Outputs without standarization: \" + str(outputs))\n",
        "print(\"Standarized outputs: \" + str(standardize(outputs)))"
      ],
      "metadata": {
        "id": "FKDbjdNI9vrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us mix everything, as in BO would be used."
      ],
      "metadata": {
        "id": "3ftuHWn2-W3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = generate_initial_data(20)\n",
        "inputs = data[0]\n",
        "outputs = data[1]"
      ],
      "metadata": {
        "id": "7BVQEsSR-0rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_inputs = normalize(inputs, bounds=bounds)\n",
        "standardized_outputs = standardize(outputs)\n",
        "print(\"Normalized inputs: \" + str(normalized_inputs))\n",
        "print(\"Standarized outputs: \" + str(standardized_outputs))"
      ],
      "metadata": {
        "id": "dx9MGiu--YwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now embed this code into the previous BO loop. Watch out! We know the real bounds but we need to \"lie\" to the Gaussian process and make it think that the bounds are $[0,1]$ for the inputs and a Z(0,1) for the outputs. Let us cheat a bit the happy ignorant GP using the function unnormalize to evaluate the objective function there, while the GP thinks that is normalized into the $[0,1]$ cube: "
      ],
      "metadata": {
        "id": "_kiUrIff-_Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_functions(single_model, best_init_y, best_candidate, candidate_acq_fun, iteration, previous_observations, previous_values, bounds, best_candidate_normalized):\n",
        "  predictive_mean, predictive_variance = compute_predictive_distribution(single_model, best_init_y, l_bound=0, h_bound=1)\n",
        "  print_predictive_mean(predictive_mean, predictive_variance, iteration, suggested=candidate_acq_fun, old_obs=previous_observations, old_values=previous_values, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "  acq_fun = compute_acquisition_function(single_model, best_init_y, l_bound=0, h_bound=1)\n",
        "  print_acquisition_function(acq_fun, iteration, suggested=candidate_acq_fun, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "  print_objective_function(best_candidate, iteration, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "\n",
        "def get_next_points_and_visualize_norm(init_x, init_y, best_init_y, normalized_bounds, iteration, previous_observations, previous_values, bounds, n_points=1):\n",
        "  single_model = SingleTaskGP(init_x, init_y)\n",
        "  mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
        "  fit_gpytorch_model(mll)\n",
        "\n",
        "  EI = ExpectedImprovement(model=single_model, best_f=best_init_y, maximize=False)\n",
        "  \n",
        "  candidates, _ = optimize_acqf(acq_function=EI, bounds=normalized_bounds, q=n_points, num_restarts=200, raw_samples=512, options={\"batch_limit\": 5, \"maxiter\": 200})\n",
        "  best_candidate = unnormalize(init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0], bounds=normalized_bounds)\n",
        "  best_candidate_normalized = init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0]\n",
        "\n",
        "  visualize_functions(single_model, best_init_y, best_candidate, unnormalize(candidates, bounds=bounds), iteration, previous_observations, previous_values, bounds, best_candidate_normalized)\n",
        "\n",
        "  return candidates"
      ],
      "metadata": {
        "id": "-kXw_5uKC2Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First the version without visualizations. First 5 points are chosen at random. Just to see whether everything is OK. Watch out!!! The bounds must be also normalized now... easy to not have that into account."
      ],
      "metadata": {
        "id": "rAax0WDpyhIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations=10\n",
        "\n",
        "init_x, init_y, best_init_y = generate_initial_data(5)\n",
        "bounds = torch.tensor([[lower_bound], [upper_bound]])\n",
        "normalized_bounds = torch.tensor([[0.0], [1.0]])\n",
        "init_x_normalized = normalize(init_x, bounds=bounds)\n",
        "init_y_standardized = standardize(init_y)\n",
        "best_init_y_standardized = init_y_standardized.min().item()\n",
        "\n",
        "\n",
        "candidates=[]\n",
        "results=[]\n",
        "for i in range(n_iterations):\n",
        "  print(f\"Number of iterations done: {i}\")\n",
        "  normalized_new_candidates = get_next_points(init_x_normalized, init_y_standardized, best_init_y_standardized, normalized_bounds, 1)\n",
        "  new_candidates = unnormalize(normalized_new_candidates, bounds=bounds)\n",
        "  new_results = target_function(new_candidates).unsqueeze(-1)\n",
        "\n",
        "  print(f\"New candidates are: {new_candidates}\")\n",
        "  init_x = torch.cat([init_x, new_candidates])\n",
        "  init_y = torch.cat([init_y, new_results])\n",
        "  init_x_normalized = normalize(init_x, bounds=bounds)\n",
        "  init_y_standardized = standardize(init_y)\n",
        "\n",
        "  best_init_y = init_y.min().item()\n",
        "  best_init_y_standardized = init_y_standardized.min().item()\n",
        "  print(f\"Best point performs this way: {best_init_y}\")\n",
        "  candidates.append(float(normalized_new_candidates[0][0]))\n",
        "  results.append(float(standardize(new_results[0][0])))"
      ],
      "metadata": {
        "id": "iRDFnxq-ygue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems OK! Bayesian optimization is exploring the center of our 1D sphere, just as we want it. So the GP now works!!! Now the version with visualizations, some little tricks must be done, as specifing normalized bounds for optimization but bounds for visualization, unnormalize the best candidate and so on..."
      ],
      "metadata": {
        "id": "e1SqlaPzylvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations=10\n",
        "\n",
        "init_x, init_y, best_init_y = generate_initial_data(5)\n",
        "bounds = torch.tensor([[lower_bound], [upper_bound]])\n",
        "normalized_bounds = torch.tensor([[0.0], [1.0]])\n",
        "init_x_normalized = normalize(init_x, bounds=bounds)\n",
        "init_y_standardized = standardize(init_y)\n",
        "best_init_y_standardized = init_y_standardized.min().item()\n",
        "\n",
        "candidates=[]\n",
        "results=[]\n",
        "for i in range(n_iterations):\n",
        "  print(f\"Number of iterations done: {i}\")\n",
        "  normalized_new_candidates = get_next_points_and_visualize_norm(init_x_normalized, init_y_standardized, best_init_y_standardized, normalized_bounds, i, init_x, init_y, bounds, 1)\n",
        "  new_candidates = unnormalize(normalized_new_candidates, bounds=bounds)\n",
        "  new_results = target_function(new_candidates).unsqueeze(-1)\n",
        "\n",
        "  print(f\"New candidates are: {new_candidates}\")\n",
        "  init_x = torch.cat([init_x, new_candidates])\n",
        "  init_y = torch.cat([init_y, new_results])\n",
        "  init_x_normalized = normalize(init_x, bounds=bounds)\n",
        "  init_y_standardized = standardize(init_y)\n",
        "\n",
        "  best_init_y = init_y.min().item()\n",
        "  best_init_y_standardized = init_y_standardized.min().item()\n",
        "  print(f\"Best point performs this way: {best_init_y}\")\n",
        "  candidates.append(float(normalized_new_candidates[0][0]))\n",
        "  results.append(float(standardize(new_results[0][0])))"
      ],
      "metadata": {
        "id": "FFeKZJgo_Og7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}