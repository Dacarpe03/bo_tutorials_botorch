{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BOTorch tutorial\n",
        "Here we illustrate how to tune a machine learning hyper-parameter using BOTorch."
      ],
      "metadata": {
        "id": "Ut_7qUec_fPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we install BOTorch"
      ],
      "metadata": {
        "id": "lUHXZaZ-EBaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install botorch"
      ],
      "metadata": {
        "id": "DRylSzQ1EC0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries. We will just tune the alpha regularization term of a MLPClassifier on a synthetic classification problem."
      ],
      "metadata": {
        "id": "aLB1XvRk_cp0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_k6vXoy_Y4p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import plotly\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "seed=1\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective function: Estimation of the generalization error measured by the lower confidence bound of the accuracy loss function of a multilayer perceptron whose weights are estimated via a k-fold cross validation on a synthetic dataset of 20 variables $\\mathbf{X}$ to perform binary classification of the dummy variable $y$. We do not need to shuffle the dataset as it is synthetic, but watch out of that for real problems."
      ],
      "metadata": {
        "id": "sO5iEVhA_n8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples=1000, random_state=1)"
      ],
      "metadata": {
        "id": "6VmkhXB4-VTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a low fidelity of the real performance brought by this classifier here as I limit the number of epochs of the MLP to 150 for didactic purposes. Also, the variance of the classifier can be improved by setting a higher number of folds, but I also set it to 3 for didactic purposes. We set the synthetic problem to be common for all the experiments. "
      ],
      "metadata": {
        "id": "9GS_cR4tD9UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target_function(alpha_regularizer=0.0001, seed=1):\n",
        "  X, y = make_classification(n_samples=1000, random_state=1)\n",
        "  clf = MLPClassifier(random_state=seed, max_iter=150, alpha=alpha_regularizer)\n",
        "  scores = cross_val_score(clf, X, y, cv=3)\n",
        "  obj_function = scores.mean() - scores.std()\n",
        "  result = []\n",
        "  result.append(obj_function)\n",
        "  print(\"Estimation of the accuracy of the MLP on the synthetic dataset done\")\n",
        "  return result"
      ],
      "metadata": {
        "id": "pBQXY7oZ_o3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the accuracy function of the alpha hyper-parameter, that we want to maximize. Here we set the resolution to 100 for didactic purposes. As the 3-fold CV estimates three models to estimate the value of the accuracy parameter that represents the generalization error, we are really training 300 models here. So this may be computationally costly."
      ],
      "metadata": {
        "id": "F6FwE1eYABoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "lower_bound = 0.000001\n",
        "upper_bound = 0.1\n",
        "resolution = 50\n",
        "#resolution = 100\n",
        "x = np.linspace(lower_bound, upper_bound, resolution)\n",
        "x_new = x.reshape((resolution,-1))\n",
        "z = np.array([target_function(alpha) for alpha in x]).reshape((resolution))\n",
        "\n",
        "data = go.Scatter(x=x, y=z, line_color=\"#FE73FF\")\n",
        "\n",
        "fig = go.Figure(data=data)\n",
        "fig.update_layout(title=\"Accuracy estimated by 3-fold-CV\", xaxis_title=\"Alpha regularizer value\", yaxis_title=\"Accuracy\")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ZlqngMKbAEA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate some data. Example of 10 random points from the input space, normalized into the range."
      ],
      "metadata": {
        "id": "H3VlN1QSBIJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = torch.rand(3, 1) * (upper_bound - lower_bound) + lower_bound\n",
        "print(train_x.min())\n",
        "print(train_x.max())"
      ],
      "metadata": {
        "id": "-FzuqL7TBJSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we compute the latent function $f(x)$. The true evaluation would be contaminated. $y = f(x) + \\epsilon \\quad s.t. \\quad \\epsilon \\approx N(0, \\sigma)$"
      ],
      "metadata": {
        "id": "FSg7ptujBZkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exact_obj = np.array([target_function(float(x)) for x in train_x])\n",
        "exact_obj"
      ],
      "metadata": {
        "id": "cI-7IyqPBOru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see which is the best observed value so far. Assuming maximization as we are dealing with a lower bound on the accuracy."
      ],
      "metadata": {
        "id": "WDSXYHFFBnI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_observed_value = exact_obj.max().item()\n",
        "best_observed_value"
      ],
      "metadata": {
        "id": "LjNExoa9BpmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wrap all of the previous code into a function to be used freely"
      ],
      "metadata": {
        "id": "ibI-21seCe6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_initial_data(n=10):\n",
        "  train_x = torch.rand(n, 1, dtype=torch.double) * (upper_bound - lower_bound) + lower_bound\n",
        "  exact_obj = torch.tensor([target_function(float(alpha)) for alpha in train_x])\n",
        "  best_observed_value = exact_obj.max().item()\n",
        "  return train_x, exact_obj, best_observed_value"
      ],
      "metadata": {
        "id": "QX6Y4KY4ChtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_initial_data(3)"
      ],
      "metadata": {
        "id": "joClmvvSCz5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now invoke this function to start the BO iteration and set the bounds of the 1-D $f(x) : x \\in [-0.1,0.1]$"
      ],
      "metadata": {
        "id": "q8merVh3DEZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_x, init_y, best_init_y = generate_initial_data(3)\n",
        "bounds = torch.tensor([[lower_bound], [upper_bound]]) #bounds for 2D: torch.tensor([[0., 1.], [10.,2.]]) "
      ],
      "metadata": {
        "id": "mqXQD_GxDGxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now declare a set of functions that will help us to visualize all the components of Bayesian optimization."
      ],
      "metadata": {
        "id": "NNgRjAMYEc75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from botorch.acquisition.analytic import ExpectedImprovement\n",
        "from botorch.optim import optimize_acqf\n",
        "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch import fit_gpytorch_model\n",
        "\n",
        "def print_objective_function(best_candidate, iteration, l_bound=0, h_bound=1):\n",
        "  x = np.linspace(l_bound, h_bound, 100)\n",
        "  x_new = x.reshape((100,-1))\n",
        "  z = target_function(x_new)\n",
        "\n",
        "  data = go.Scatter(x=x, y=z, line_color=\"#FE73FF\")\n",
        "\n",
        "  fig = go.Figure(data=data)\n",
        "  fig.update_layout(title=\"Objective function. Iteration \" + str(iteration), xaxis_title=\"input\", yaxis_title=\"output\")\n",
        "  fig.add_vline(x=best_candidate, line_width=3, line_color=\"red\")\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "def compute_acquisition_function(single_model, best_init_y, l_bound=-2., h_bound=10., resolution=1000):\n",
        "  linspace = torch.linspace(l_bound, h_bound, steps=resolution)\n",
        "  x_test = torch.tensor([linspace[0]]).unsqueeze(-1)\n",
        "  EI = ExpectedImprovement(model=single_model, best_f=best_init_y, maximize=True)\n",
        "  result = []\n",
        "  for x in linspace:\n",
        "    x_test = torch.tensor([x]).unsqueeze(-1)\n",
        "    result.append(EI(x_test))\n",
        "  return torch.tensor(result)\n",
        "\n",
        "def print_acquisition_function(acq_fun, iteration, l_bound=-2., h_bound=10., resolution=1000, suggested=None):\n",
        "  x = torch.linspace(l_bound, h_bound, steps=resolution).detach().numpy()\n",
        "  x_new = x.reshape((resolution,-1))\n",
        "  z = acq_fun\n",
        "  max_acq_fun = x[((acq_fun == acq_fun.max().item()).nonzero(as_tuple=True)[0])]\n",
        "  data = go.Scatter(x=x, y=z, line_color=\"yellow\")\n",
        "\n",
        "  fig = go.Figure(data=data)\n",
        "  fig.update_layout(title=\"Expected Improvement acquisition function. Iteration \" + str(iteration), xaxis_title=\"input\", yaxis_title=\"output\")\n",
        "  if(suggested==None):\n",
        "    fig.add_vline(x=max_acq_fun, line_width=3, line_color=\"red\")\n",
        "  else:\n",
        "    fig.add_vline(x=float(suggested[0][0]), line_width=3, line_color=\"red\")\n",
        "  fig.show()\n",
        "\n",
        "def compute_predictive_distribution(single_model, best_init_y, l_bound=-2., h_bound=10., resolution=1000):\n",
        "  linspace = torch.linspace(l_bound, h_bound, steps=resolution)\n",
        "  x_test = torch.tensor([linspace[0]]).unsqueeze(-1)\n",
        "  result = []\n",
        "  variances = []\n",
        "  for x in linspace:\n",
        "    x_test = torch.tensor([x]).unsqueeze(-1)\n",
        "    result.append(single_model.posterior(x_test).mean)\n",
        "    variances.append(single_model.posterior(x_test).variance)\n",
        "  return torch.tensor(result), torch.tensor(variances)\n",
        "\n",
        "def print_predictive_mean(predictive_mean, predictive_variance, iteration, l_bound=-2., h_bound=10., resolution=1000, suggested=None, old_obs=[], old_values=[]):\n",
        "  x = torch.linspace(l_bound, h_bound, steps=resolution).detach().numpy()\n",
        "  x_new = x.reshape((resolution,-1))\n",
        "  z = predictive_mean\n",
        "  max_predictive_mean = x[((predictive_mean == predictive_mean.max().item()).nonzero(as_tuple=True)[0])]\n",
        "\n",
        "  fig = go.Figure()\n",
        "\n",
        "  fig.add_trace(go.Scatter(x=x, y= predictive_mean + np.sqrt(predictive_variance),\n",
        "                                     mode='lines',\n",
        "                                     line=dict(color=\"#19D3F3\",width =0.1),\n",
        "                                     name='upper bound'))\n",
        "  fig.add_trace(go.Scatter(x=x, y= predictive_mean,\n",
        "                         mode='lines',\n",
        "                         line=dict(color=\"blue\"),\n",
        "                         fill='tonexty',\n",
        "                         name='predictive mean'))\n",
        "  fig.add_trace(go.Scatter(x=x, y= predictive_mean - np.sqrt(predictive_variance),\n",
        "                         mode='lines',\n",
        "                         line=dict(color=\"blue\", width =0.1),\n",
        "                         fill='tonexty',\n",
        "                         name='lower bound'))\n",
        "  \n",
        "  \n",
        "  \n",
        "  fig.update_layout(title=\"GP Predictive distribution. Iteration \" + str(iteration), xaxis_title=\"input\", yaxis_title=\"output\", showlegend=False)\n",
        "\n",
        "  if(suggested==None):\n",
        "    fig.add_vline(x=max_predictive_mean, line_width=3, line_color=\"red\")\n",
        "  else:\n",
        "    fig.add_vline(x=float(suggested[0][0]), line_width=3, line_color=\"red\")  \n",
        "\n",
        "  if(len(old_obs)>0):\n",
        "    fig.add_trace(go.Scatter(x=old_obs, y=old_values, mode = 'markers', marker_color=\"black\", marker_size=10))\n",
        "\n",
        "  fig.show()\n",
        "\n",
        "def visualize_functions(single_model, best_init_y, best_candidate, candidate_acq_fun, iteration, previous_observations, previous_values, bounds, best_candidate_normalized):\n",
        "  predictive_mean, predictive_variance = compute_predictive_distribution(single_model, best_init_y, l_bound=0, h_bound=1)\n",
        "  print_predictive_mean(predictive_mean, predictive_variance, iteration, suggested=candidate_acq_fun, old_obs=previous_observations, old_values=previous_values, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "  acq_fun = compute_acquisition_function(single_model, best_init_y, l_bound=0, h_bound=1)\n",
        "  print_acquisition_function(acq_fun, iteration, suggested=candidate_acq_fun, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "  #Very expensive to compute here, just see that the predictive mean is just like the obj function! print_objective_function(best_candidate, iteration, l_bound=bounds[0][0], h_bound=bounds[1][0])\n",
        "\n",
        "def get_next_points_and_visualize_norm(init_x, init_y, best_init_y, normalized_bounds, iteration, previous_observations, previous_values, bounds, n_points=1):\n",
        "  single_model = SingleTaskGP(init_x, init_y)\n",
        "  mll = ExactMarginalLogLikelihood(single_model.likelihood, single_model)\n",
        "  fit_gpytorch_model(mll)\n",
        "\n",
        "  EI = ExpectedImprovement(model=single_model, best_f=best_init_y, maximize=True)\n",
        "  \n",
        "  candidates, _ = optimize_acqf(acq_function=EI, bounds=normalized_bounds, q=n_points, num_restarts=200, raw_samples=512, options={\"batch_limit\": 5, \"maxiter\": 200})\n",
        "  best_candidate = unnormalize(init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0], bounds=normalized_bounds)\n",
        "  best_candidate_normalized = init_x[((init_y == best_init_y).nonzero(as_tuple=True)[0])][0][0]\n",
        "\n",
        "  visualize_functions(single_model, best_init_y, best_candidate, unnormalize(candidates, bounds=bounds), iteration, previous_observations, previous_values, bounds, best_candidate_normalized)\n",
        "\n",
        "  return candidates"
      ],
      "metadata": {
        "id": "XS4zdkHyjGWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now embed this code into the BO loop. First three points are drawn at random."
      ],
      "metadata": {
        "id": "_kiUrIff-_Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations=10\n",
        "\n",
        "init_x, init_y, best_init_y = generate_initial_data(3)\n",
        "bounds = torch.tensor([[lower_bound], [upper_bound]])\n",
        "normalized_bounds = torch.tensor([[0.0], [1.0]])\n",
        "init_x_normalized = normalize(init_x, bounds=bounds)\n",
        "init_y_standardized = standardize(init_y)\n",
        "best_init_y_standardized = init_y_standardized.max().item()\n",
        "\n",
        "candidates=[]\n",
        "results=[]\n",
        "for i in range(n_iterations):\n",
        "  print(f\"Number of iterations done: {i}\")\n",
        "  normalized_new_candidates = get_next_points_and_visualize_norm(init_x_normalized, init_y_standardized, best_init_y_standardized, normalized_bounds, i, init_x, init_y, bounds, 1)\n",
        "  new_candidates = unnormalize(normalized_new_candidates, bounds=bounds)\n",
        "  new_results = torch.tensor([target_function(float(new_candidates))])\n",
        "\n",
        "  print(f\"New candidates are: {new_candidates}\")\n",
        "  init_x = torch.cat([init_x, new_candidates])\n",
        "  init_y = torch.cat([init_y, new_results])\n",
        "  init_x_normalized = normalize(init_x, bounds=bounds)\n",
        "  init_y_standardized = standardize(init_y)\n",
        "\n",
        "  best_init_y = init_y.max().item()\n",
        "  best_init_y_standardized = init_y_standardized.max().item()\n",
        "  print(f\"Best point performs this way: {best_init_y}\")\n",
        "  candidates.append(float(normalized_new_candidates[0][0]))\n",
        "  results.append(float(standardize(new_results[0][0])))"
      ],
      "metadata": {
        "id": "FFeKZJgo_Og7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}